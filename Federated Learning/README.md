# Federated Learning

[Agnostic Federated Learning](https://arxiv.org/pdf/1902.00146.pdf)

- By presenting data-dependent Rademacher complexity guarantees and a fast stochastic optimization algorithm, the centralized model of the new framework of agnostic federated learning creates is optimized for any target distribution formed by a mixture of the client distributions and yields a notion of fairness.

[Applied Federated Learning: Improving Google Keyboard Query Suggestions](https://arxiv.org/pdf/1812.02903.pdf)

- By using federated learning in a commercial, global-scale setting to improve virtual keyboard search suggestion quality without direct access to the underlying user data, this paper's design demonstrate how federated learning can be applied end-to-end to both improve user experiences and enhance user privacy.

[Can You Really Backdoor Federated Learning?](https://arxiv.org/pdf/1911.07963.pdf)

- By allowing non-malicious clients to have correctly labeled samples from the targeted tasks, this paper's comprehensive study of backdoor attacks and defenses shows that the attack performance depends on the fraction of adversaries and the targeted task complexity, while norm clipping and weak differential privacy mitigate the attacks.

[Client Selection for Federated Learning with Heterogeneous Resources in
Mobile Edge](https://arxiv.org/pdf/1804.08333.pdf)

- By solving a client selection problem with resource constraints, FedCS mitigates the resource heterogeneity and performs FL efficiently while actively managing clients based on their resource conditions.

[Communication-Efficient Learning of Deep Networks from Decentralized Data](https://arxiv.org/pdf/1602.05629.pdf)

- By leaving the training data distributed on the mobile devices and learns a shared model by aggregating locally-computed updates, Federated Learning is robust to the unbalanced and non-IID data distributions.

[Federated Evaluation and Tuning for On-Device Personalization: System Design & Applications](https://arxiv.org/pdf/2102.08503.pdf)

- By creating a federated task processing system, this paper's design supports federated evaluation and tuning of on-device ML systems with personalization at scale to highlight application specific solutions.

[Federated Hyperparameter Tuning: Challenges, Baselines, and Connections to Weight-Sharing]()

- By making connection to NAS technique of weight sharing, FedEx can accelerate federated hyperparameter tuning.

[Federated Learning with Matched Averaging](https://arxiv.org/pdf/2002.06440.pdf)

- By constructing the shared global model in a layer-wise manner through matching and averaging hidden elements with similar feature extraction signatures, Federated matched averaging (FedMA) outperforms popular state-of-the-art federated learning algorithms and reduces the overall communication burden.

[Federated Learning: Challenges, Methods, and Future Directions](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9084352)

- This paper discusses the unique characteristics and challenges of federated learning, provides a broad overview of current approaches, and outlines several directions of future work that are relevant to a wide range of research communities.

[Federated Learning: Strategies for Improving Communication Efficiency](https://arxiv.org/pdf/1610.05492.pdf)

- By two proposed methods, this paper's design reduces the uplink communication costs on the application of training a deep neural network to perform image classification.

[Federated Optimization in Heterogeneous Networks](https://arxiv.org/pdf/1812.06127v4.pdf)

- By generalization and re-parametrization of FedAvg, FedProx guarantees convergence for statistical heterogeneity and adhering to device-level systems constraints for systems heterogeneity, thus demonstrates significantly more stable and accurate convergence.

[Federated Optimization: Distributed Optimization Beyond the Datacenter](https://arxiv.org/pdf/1511.03575.pdf)

- By using the setting of Federated Optimization, this paper's design improves the performance for federated learning that keeps the training data locally on users&#039; mobile devices rather than logging it to a data center for training and no device has a representative sample of the overall distribution.

[Federated Variance-Reduced Stochastic Gradient Descent with Robustness to Byzantine Attacks](https://arxiv.org/pdf/1912.12716.pdf)

- By relying on the geometric median to aggregate the corrected stochastic gradients, the novel Byzantine attack resilient distributed (Byrd-) SAGA approach is robust to various Byzantine attacks for distributed finite-sum optimization for learning over multiple workers.

[How To Backdoor Federated Learning](https://arxiv.org/pdf/1807.00459.pdf)

- By using model replacement that introduces hidden backdoor functionality into the joint global model and incorporation evasion into the attacker&#039;s loss function, this paper's design of a new model-poisoning methodology greatly outperforms data poisoning and evades anomaly detection-based defenses.

[Overcoming Forgetting in Federated Learning on Non-IID Data](https://arxiv.org/pdf/1910.07796.pdf)

- By adapting a solution for catastrophic forgetting to Federated Learning building on an analogy with Lifelong Learning through adding a penalty term to the loss function, this paper's design tackles the problem where local models drift apart, inhibiting learning.

[Towards Federated Learning at Scale: System Design](https://arxiv.org/pdf/1902.01046.pdf)

- By building a scalable production system for Federated Learning in the domain of mobile devices, based on TensorFlow, this paper describe the resulting high-level design, sketch some of the challenges and their solutions, and touch upon the open problems and future directions.
