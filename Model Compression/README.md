# Model Compression

[AMC: AutoML for Model Compression and Acceleration on Mobile Devices](https://arxiv.org/pdf/1802.03494.pdf)

- By using reinforcement learning to provide the model compression policy, AutoML for Model Compression (AMC) outperforms conventional rule-based compression policy by having higher compression ratio, better preserving the accuracy and freeing human labor.

[Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding](https://arxiv.org/pdf/1510.00149.pdf)

- By introducing a three stage pipeline of pruning, trained quantization and Huffman coding, "deep compression" is able to reduce the storage requirement of neural networks without affecting the accuracy. 

[EIE: Efficient Inference Engine on Compressed Deep Neural Network](https://arxiv.org/pdf/1602.01528.pdf)

- By performing inference on the "deep compression" network model and using weight sharing for sparse matrix-vector multiplication, energy efficient inference engine (EIE) obtains better throughput, energy efficiency, and area efficiency.

[ESE: Efficient Speech Recognition Engine with Sparse LSTM on FPGA](https://arxiv.org/pdf/1612.00694.pdf)

- By using a load-balance-aware pruning method of pruning and quantization, a scheduler for parallelism and LSTM data flow, and a hardware architecture called ESE, this paper's design can compress the LSTM model size, maintain prediction accuracy, and achieve high hardware utilization.

[HAQ: Hardware-Aware Automated Quantization With Mixed Precision](https://arxiv.org/pdf/1811.08886.pdf)

- By using reinforcement learning with the feedback from the hardware accelerator, Hardware-Aware Automated Quantization (HAQ) is fully automated and can specialize the quantization policy for different neural network architectures and hardware architectures.

