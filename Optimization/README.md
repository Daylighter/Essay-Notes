# Optimization

## Dynamic model

[Relay: A New IR for Machine Learning Frameworks](http://dl.acm.org/ft_gateway.cfm?id=3211348&ftid=1979148&dwn=1)

- By designing a purely-functional, statically-typed language, Relay, a new high-level intermediate representation (IR), is efficient, expressive, and portable across an array of heterogeneous hardware devices with the use of the open source NNVM compiler framework, which powers Amazonâ€™s deep learning framework MxNet.

[Cortex: A Compiler for Recursive Deep Learning Models](https://arxiv.org/pdf/2011.01383.pdf)

- By performing high-level graph optimizations such as kernel fusion and low level kernel optimizations such as those found in vendor libraries, Cortex, a compiler-based approach, can generate highly-efficient code for recursive models for low latency inference on end-to-end optimizations.

[Nimble: Efficiently Compiling Dynamic Neural Networks for Model Inference](https://arxiv.org/pdf/2006.03031.pdf)

- By introducing a dynamic type system, a set of dynamism-oriented optimizations, and a light-weight virtual machine runtime, Nimble, a high-performance and flexible system, can optimize, compile, and execute dynamic neural networks on multiple platforms.

## Memory optimization

